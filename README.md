# Poetry Chatbot — Educator + Poet

Language models produce poetry that is formally competent but stylistically empty—lacking the distinctive voice that makes verse matter. This project implements **synthetic education**: a methodology that trains models not on poems but on poetry instruction—critique, revision dialogue, and craft lessons—generated by a frontier model in a consistent pedagogical persona. A two-phase pipeline produces a structured corpus that fine-tunes separate **Educator** (mentor/critic) and **Poet** (generator) models, which operate in a multi-agent revision loop at inference. Both models (Llama 3.1 8B, QLoRA, Q4_K_M) run locally on consumer hardware. **RevFlux**, a process-oriented benchmark, measures revision dynamics rather than output quality. The trained system exhibits substantive critique-responsive revision, coarse-to-fine editing dynamics, and prompt-sensitive behaviour absent in the vanilla baseline—demonstrating that encoding pedagogical process rather than poetic product enables qualitatively different revision behaviour. See [docs/ARCHITECTURE.md](docs/ARCHITECTURE.md) and [docs/DESIGN.md](docs/DESIGN.md).

## Model

**Base: Llama 3.1 8B Instruct** (`meta-llama/Llama-3.1-8B-Instruct`). Two Q4_K_M models (educator + rhyme-trained poet, ~4.5GB each) fit on 24GB Mac Mini.

## Setup

```bash
pip install -r requirements.txt
# .env: API key for frontier model (data generation), HF_TOKEN (Hugging Face)
modal token set
modal secret create huggingface-secret HF_TOKEN=your_token
```

## Data prep

Add raw poetry to `data/raw/good/` and `data/raw/bad/`. Format: `.json` or `.jsonl` with `{author, title, poem}` per item; `.txt` as plain poem. Contrastive learning: bad poems paired with good for comparisons.

## Full pipeline

| Step | What | Frontier API | Local |
|------|------|--------------|-------|
| 1 | Hard tasks (critiques, comparisons, revision_briefs) | yes | — |
| 2 | Train interim educator on seed | 0 | Modal GPU |
| 3 | Export + download interim educator | 0 | Modal + local |
| 4 | Local educator generates briefs, autopsies, lessons | 0 | llama.cpp |
| 5 | Poet pairs | yes | — |
| 6 | Rhyme pairs + approval examples | yes | — |
| 7 | Prepare full training data (educator + poet + rhyme) | 0 | Combines JSONL |
| 8 | Train final educator + poet + rhyme poet | 0 | Modal GPU |
| 9 | Export + download final models | 0 | Modal + local |

Seed data (critiques, comparisons, revision_briefs, poet_pairs, rhyme_pairs) is generated by a **frontier model** in the educator persona. Briefs, autopsies, and lessons are then generated by the **interim educator** (trained on seed, run locally). The workflow also runs `generate_dialogues.py` and `generate_approval_examples.py` before the final prepare step. The **Poet** used at inference is the rhyme-trained poet (form-specific brief+poem+critique data + curated strong-rhyme corpus).

### 1. Data generation (frontier API)

**Hard tasks:**
| Script | Input | Output |
|--------|-------|--------|
| `generate_critiques_seed` | raw good + bad | critiques_seed.jsonl |
| `generate_comparisons` | good + bad pairs | comparisons.jsonl |
| `generate_revision_briefs` | critiques_seed | revision_briefs_seed.jsonl |
| `generate_poet_pairs` | briefs + revision_briefs | pairs.jsonl |
| `generate_rhyme_pairs` | form-specific | rhyme poet + educator data |
| `generate_approval_examples` | — | educator approval/rejection examples |

**Easier (interim educator or frontier):**
| Script | Input | Output |
|--------|-------|--------|
| `generate_briefs` | raw good | briefs.jsonl |
| `generate_autopsies` | raw bad | autopsies.jsonl |
| `generate_lessons` | craft questions | lessons.jsonl |

```bash
# Full run
python scripts/data_generation/generate_critiques_seed.py --limit-good 200
python scripts/data_generation/generate_comparisons.py
python scripts/data_generation/generate_revision_briefs.py --limit 50
python scripts/data_generation/prepare_training_data.py --interim-educator --educator-only
# Upload, train interim educator, export, download; then:
python scripts/data_generation/generate_with_local_educator.py --all --limit-briefs 200 --limit-lessons 10
python scripts/data_generation/generate_poet_pairs.py
python scripts/data_generation/generate_dialogues.py
python scripts/data_generation/generate_rhyme_pairs.py --replace
python scripts/data_generation/generate_approval_examples.py --replace
```

### 2. Prepare training data

```bash
python scripts/data_generation/prepare_training_data.py [--educator-only] [--poet-only] [--min-samples N] [--seed N]
python scripts/data_generation/prepare_rhyme_training_data.py [--general-frac 0.2]
```

Combines outputs into chat format. Rhyme data: curated strong-rhyme poems + form-specific pairs (80% rhyme / 20% general by default). `--min-samples N`: requires ≥N examples; if set, caps train/valid (quick test).

### 3. Upload to Modal

```bash
python scripts/modal/upload_data.py
```

Uploads `data/educator_training/`, `data/poet_training/`, and `data/rhyme_training/` to `poetry-data` volume.

### 4. Train + export (Modal)

```bash
# Educator
modal run scripts/modal/train_educator.py [--num-epochs-override N]
modal run scripts/modal/export_gguf.py::export_educator

# Poet (general) + Rhyme poet (inference default)
modal run scripts/modal/train_poet.py [--num-epochs-override N]
modal run scripts/modal/train_rhyme_poet.py [--num-epochs-override N]
modal run scripts/modal/export_gguf.py::export_poet
modal run scripts/modal/export_gguf.py::export_poet_rhyme

# Or orchestrate
modal run scripts/modal/modal_app.py [--educator-only] [--poet-only] [--train-only] [--num-epochs N]
```

### 5. Download + inference

```bash
modal volume get --force poetry-gguf llama3.1-8b-educator-Q4_K_M.gguf models/
modal volume get --force poetry-gguf llama3.1-8b-poet_rhyme-Q4_K_M.gguf models/

python scripts/inference/pipeline.py "Write a poem about winter light" [--config PATH]
```

Default config uses the rhyme-trained poet. **Reference chat server:** `python serve_gpm.py [port]` (default 11435). `POST /api/chat` with JSON `{ "messages": [ {"role":"user","content":"..."} ] }` for streaming (application/x-ndjson).

## Full workflow

```bash
./scripts/run_full_workflow.sh
```

Runs: seed data (frontier) → prepare interim educator → train interim educator on Modal → export/download interim GGUF → local educator generates briefs, autopsies, lessons → poet_pairs → generate_dialogues → rhyme_pairs + approval_examples → prepare full data (including rhyme) → upload → train educator + poet + rhyme poet → export → download. Use `--skip-generation` if `train.jsonl` already exists.

## First test

```bash
./scripts/run_first_test.sh
```

Minimal data → prepare (`--min-samples 5`) → upload → train both educator and poet (1 epoch) → export → inference. Skips if `train.jsonl` exists.

## Rhyme poet (inference default)

The poet used in the revision loop is the **rhyme-trained poet**. Data: (1) **Curated:** `select_strong_rhyme_poems.py` on good poems → `data/annotated/strong_rhyme_poems.jsonl`; `prepare_rhyme_training_data.py` builds `data/rhyme_training/` (strong-rhyme + general mix). (2) **Frontier-generated:** `generate_rhyme_pairs.py` produces form-specific brief+poem+critique pairs. Training: `config/rhyme_training.yaml`, `scripts/modal/train_rhyme_poet.py`. Eval: `rhyme_analyzer.py`, `meter_analyzer.py`, `form_registry.py`.

## Config

| File | Purpose |
|------|---------|
| `config/educator_training.yaml` | QLoRA (r=64, α=128, 4 epochs, max_seq 1024) |
| `config/poet_training.yaml` | QLoRA general poet |
| `config/rhyme_training.yaml` | Rhyme poet QLoRA / data mix (inference default) |
| `config/export_pipeline.yaml` | Merge + GGUF Q4_K_M |
| `config/inference_config.yaml` | llama.cpp n_ctx, temperature; educator + poet_rhyme paths |
| `config/model_registry.yaml` | Base model registry and fit guidance |
| `config/rev_flux_models.yaml` | RevFlux: trained vs vanilla Ollama configs |
| `config/data_generation.yaml` | Data generation defaults |

## Structure

```
data/raw/{good,bad}/     # Poetry input
data/annotated/          # Critiques, autopsies, comparisons, strong_rhyme_poems
data/educator_training/  # train.jsonl, valid.jsonl
data/poet_training/      # train.jsonl, valid.jsonl
data/rhyme_training/     # Rhyme poet train/valid (inference default poet)
persona/                 # educator_neutral.txt, persona_condensed.txt
adapters/                # LoRA adapters (e.g. poet_rhyme)
config/                  # YAML configs
scripts/{benchmarks,data_generation,modal,eval,inference,training}/
serve_gpm.py             # Reference HTTP chat server (POST /api/chat)
```

**Alternative inference:** `scripts/inference/swapping_pipeline.py` loads one model at a time (e.g. 32B poet). **Vanilla comparison:** pipeline supports `educator_model_override` and `poet_model_override` (e.g. `ollama:llama3.1:8b`) for trained GGUF vs base.

## Benchmarking / eval

### RevFlux (revision process benchmark)

`scripts/benchmarks/rev_flux/` measures **process dynamics**, not outcome quality: how much each line changes during revision, where change concentrates, and how this varies by prompt category and revision length. Only the trained model runs the revision loop; vanilla models produce initial drafts only.

```bash
python scripts/benchmarks/rev_flux/run_harness.py [--visualize]
python scripts/benchmarks/rev_flux/run_harness.py --models trained llama3.1-8b --limit 2
python scripts/benchmarks/rev_flux/run_harness.py --list-models
```

Models: `config/rev_flux_models.yaml`. See `scripts/benchmarks/rev_flux/README.md`.

### Eval scripts

| Script | Purpose |
|--------|---------|
| `rhyme_analyzer.py` | CMU-based rhyme detection: scheme, strict vs slant. Input: poem text. |
| `meter_analyzer.py` | Scansion via CMU: metrical pattern, consistency. |
| `form_registry.py` | Form definitions (sonnet, villanelle, limerick, etc.). |
| `select_strong_rhyme_poems.py` | Strong CMU-verified rhyme → `data/annotated/strong_rhyme_poems.jsonl`. |
| `voice_consistency.py` | Educator outputs: banned phrases, specificity; input JSONL with `output`/`response`/`content`. |
