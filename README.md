# Poetry Chatbot — Educator + Poet (v3)

Two-model poetry system: **Educator** (mentor/critic) + **Poet** (generator). Cloud training on Modal, local inference via llama.cpp (e.g. Mac). Training data via Claude API. See [docs/ARCHITECTURE.md](docs/ARCHITECTURE.md) for overview and [docs/DESIGN.md](docs/DESIGN.md) for full spec.

## Model choice

**Base: Qwen 2.5 7B Instruct** (`Qwen/Qwen2.5-7B-Instruct`). Apache 2.0 license, strong for nuanced text. Two Q4_K_M models (~4.5GB each) fit easily on 24GB Mac Mini.

## Setup

```bash
pip install -r requirements.txt
# .env: ANTHROPIC_API_KEY, HF_TOKEN (for Hugging Face model downloads)
modal token set
modal secret create huggingface-secret HF_TOKEN=your_token
```

## Data prep

Add raw poetry to `data/raw/good/` and `data/raw/bad/`. Format: `.json` or `.jsonl` with `{author, title, poem}` per item; `.txt` as plain poem. Contrastive learning: bad poems (~114) paired with good (~34k) for comparisons.

## Full pipeline

| Step | What | Anthropic | Local |
|------|------|-----------|-------|
| 1 | Hard tasks (critiques, comparisons, revision_briefs) | Opus (forced) | — |
| 2 | Train interim educator on seed | 0 | Modal GPU |
| 3 | Export + download interim educator | 0 | Modal + local |
| 4 | Local educator generates briefs, autopsies, lessons | 0 | llama.cpp |
| 5 | Poet pairs | Opus (forced) | — |
| 6 | Prepare full training data | 0 | Combines JSONL |
| 7 | Train final educator + poet | 0 | Modal GPU |
| 8 | Export + download final models | 0 | Modal + local |

**Anthropic for hard tasks only:** critiques, comparisons, revision_briefs, poet_pairs use Claude (force_anthropic, no local fallback). In the full workflow, briefs, autopsies, and lessons are generated by the **interim educator** (trained on seed, then run locally); you can also run `generate_briefs` / `generate_autopsies` / `generate_lessons` with Sonnet for one-off data. The workflow also runs `generate_dialogues.py` before the final prepare step.

### 1. Data generation (Claude API)

**Hard tasks (Opus):**
| Script | Input | Output |
|--------|-------|--------|
| `generate_critiques_seed` | raw good + bad | critiques_seed.jsonl (all bad + 200 good) |
| `generate_comparisons` | good + bad pairs | comparisons.jsonl |
| `generate_revision_briefs` | critiques_seed | revision_briefs_seed.jsonl |
| `generate_poet_pairs` | briefs + revision_briefs | pairs.jsonl |

**Easier (Sonnet):**
| Script | Input | Output |
|--------|-------|--------|
| `generate_briefs` | raw good | briefs.jsonl |
| `generate_autopsies` | raw bad | autopsies.jsonl |
| `generate_lessons` | craft questions | lessons.jsonl |

```bash
# Full run
python scripts/data_generation/generate_critiques_seed.py --limit-good 200
python scripts/data_generation/generate_comparisons.py
python scripts/data_generation/generate_revision_briefs.py --limit 50
python scripts/data_generation/generate_briefs.py --input data/raw/good --limit 200
python scripts/data_generation/generate_autopsies.py
python scripts/data_generation/generate_lessons.py --limit 10
python scripts/data_generation/generate_poet_pairs.py
```

### 2. Prepare training data

```bash
python scripts/data_generation/prepare_training_data.py [--educator-only] [--poet-only] [--min-samples N] [--seed N]
```

Combines outputs into chat format. `--min-samples N`: requires ≥N examples; if set, caps train/valid to N (quick test). Omit for full run.

### 3. Upload to Modal

```bash
python scripts/modal/upload_data.py
```

Uploads `data/educator_training/{train,valid}.jsonl` and `data/poet_training/{train,valid}.jsonl` to `poetry-data` volume.

### 4. Train + export (Modal)

```bash
# Educator
modal run scripts/modal/train_educator.py [--num-epochs-override N]
modal run scripts/modal/export_gguf.py::export_educator

# Poet
modal run scripts/modal/train_poet.py [--num-epochs-override N]
modal run scripts/modal/export_gguf.py::export_poet

# Or orchestrate both
modal run scripts/modal/modal_app.py [--educator-only] [--poet-only] [--train-only] [--num-epochs N]
```

### 5. Download + inference

```bash
modal volume get --force poetry-gguf qwen2.5-7b-educator-Q4_K_M.gguf models/
modal volume get --force poetry-gguf qwen2.5-7b-poet-Q4_K_M.gguf models/

python scripts/inference/pipeline.py "Write a poem about winter light" [--config PATH]
```

**Reference chat server:** Any client can call the educator via HTTP. Run `python serve_gpm.py [port]` (default 11435). `POST /api/chat` with JSON `{ "messages": [ {"role":"user","content":"..."} ] }` for streaming completion (application/x-ndjson).

## Full workflow

```bash
./scripts/run_full_workflow.sh
```

Runs: hard tasks (Opus) → prepare interim educator data → train interim educator on Modal → export/download interim GGUF → local interim educator generates briefs, autopsies, lessons → poet_pairs (Opus) → generate_dialogues → prepare full data → upload → train final educator + poet → export → download. Use `--skip-generation` if `train.jsonl` already exists.

## First test

```bash
./scripts/run_first_test.sh
```

Runs: minimal data (5 bad + 5 good critiques, 5 comparisons, 5 revision briefs, 5 briefs, 5 autopsies, 5 lessons, 10 poet pairs) → prepare (`--min-samples 5`) → upload → train both (1 epoch) → export → inference. Skips if `train.jsonl` exists.

## Optional: Rhyme-focused poet

Train a poet with stronger rhyme/meter. Two data paths: (1) **Curated from corpus:** run `select_strong_rhyme_poems.py` on good poems → `data/annotated/strong_rhyme_poems.jsonl`; then `prepare_rhyme_training_data.py` to build `data/rhyme_training/` (80% strong-rhyme + 20% general, anti-collapse). (2) **Claude-generated:** `generate_rhyme_pairs.py` produces form-specific brief+poem+critique pairs. Training: `config/rhyme_training.yaml`, `scripts/modal/train_rhyme_poet.py`. Eval/analysis: `rhyme_analyzer.py`, `meter_analyzer.py`, `form_registry.py`.

## Config

| File | Purpose |
|------|---------|
| `config/educator_training.yaml` | QLoRA (r=64, α=128, 4 epochs, max_seq 1024) |
| `config/poet_training.yaml` | QLoRA (r=64, α=128, 6 epochs, max_seq 512) |
| `config/rhyme_training.yaml` | Optional rhyme poet QLoRA / data mix |
| `config/export_pipeline.yaml` | Merge + GGUF Q4_K_M |
| `config/inference_config.yaml` | llama.cpp n_ctx, temperature, etc. |
| `config/model_registry.yaml` | Base model registry and fit guidance |
| `config/rev_flux_models.yaml` | RevFlux benchmark: trained vs vanilla Ollama model configs |
| `config/data_generation.yaml` | Data generation defaults |

## Structure

```
data/raw/{good,bad}/     # Poetry input
data/annotated/          # Claude outputs (critiques, autopsies, comparisons)
data/educator_training/  # train.jsonl, valid.jsonl
data/poet_training/      # train.jsonl, valid.jsonl
data/rhyme_training/     # optional rhyme poet data
persona/                 # educator_neutral.txt, persona_condensed.txt
adapters/                # optional LoRA adapters (e.g. poet_rhyme)
config/                  # YAML configs
scripts/{benchmarks,data_generation,modal,eval,inference,training}/
serve_gpm.py             # Reference HTTP chat server (POST /api/chat)
```

**Alternative inference:** If both models don’t fit in memory (e.g. 32B poet), use `scripts/inference/swapping_pipeline.py` to load one model at a time. **Vanilla Ollama:** the pipeline supports `educator_model_override` and `poet_model_override` (e.g. `ollama:qwen2.5:7b-instruct`) for comparing trained GGUF vs base models.

## Benchmarking / eval

### RevFlux (revision process benchmark)

`scripts/benchmarks/rev_flux/` measures **process dynamics**, not outcome quality: how much each line changes during revision, where change concentrates, and how this varies by prompt category, revision length, and model (trained GGUF vs vanilla Ollama).

```bash
# Run harness (all models, categories, revision lengths)
python scripts/benchmarks/rev_flux/run_harness.py [--visualize]

# Compare trained vs vanilla Ollama
python scripts/benchmarks/rev_flux/run_harness.py --models trained qwen2.5-7b --limit 2
python scripts/benchmarks/rev_flux/run_harness.py --list-models
```

Models: `config/rev_flux_models.yaml`. Run `ollama pull <model>` before using vanilla models. See `scripts/benchmarks/rev_flux/README.md` for full docs.

### Eval scripts

`scripts/eval/` provides deterministic and model-output checks:

| Script | Purpose |
|--------|---------|
| `rhyme_analyzer.py` | CMU Pronouncing Dictionary–based rhyme detection: scheme, rhyme quality, strict vs. slant. Input: poem text (stdin or file). |
| `meter_analyzer.py` | Scansion via CMU: metrical pattern, consistency, foot types. Input: poem text. |
| `form_registry.py` | Library of form definitions (sonnet, villanelle, limerick, etc.): rhyme scheme, line count, meter. Used by rhyme/meter analyzers and data generation. |
| `select_strong_rhyme_poems.py` | Run rhyme analysis on good poems; write those with strong CMU-verified rhyme to `data/annotated/strong_rhyme_poems.jsonl` for rhyme training. |
| `voice_consistency.py` | Check educator outputs for banned “LLM-ism” phrases and basic specificity; input JSONL with `output` or `response` or `content`. |

