# Educator Model Training â€” S3.2
# Modal QLoRA fine-tune, Qwen 2.5 7B base

base_model: "Qwen/Qwen2.5-7B-Instruct"
model_loading:
  quantization: "nf4"
  compute_dtype: "bfloat16"
  double_quant: true

lora:
  rank: 64
  alpha: 128
  dropout: 0.05
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj
  bias: "none"
  task_type: "CAUSAL_LM"

training:
  optimizer: "paged_adamw_8bit"
  learning_rate: 2e-4
  lr_scheduler: "cosine"
  warmup_ratio: 0.03
  num_epochs: 4
  per_device_batch_size: 4
  gradient_accumulation_steps: 4
  max_seq_length: 1024
  weight_decay: 0.01
  max_grad_norm: 1.0
  fp16: false
  bf16: true

checkpointing:
  save_strategy: "epoch"
  save_path: "/vol/checkpoints/educator/"
  save_total_limit: 3

evaluation:
  eval_steps: 100
  eval_dataset: "valid.jsonl"
  metric: "eval_loss"

compute:
  gpu: "A10G"
  system_ram: "32GB"
  estimated_time: "2-3 hours"
  estimated_cost: "$2.75-$3.30"
