# Poet Model Training â€” S3.3
# Modal QLoRA fine-tune, Llama 3.1 8B base (rhyme-focused)

base_model: "meta-llama/Llama-3.1-8B-Instruct"
model_loading:
  quantization: "nf4"
  compute_dtype: "bfloat16"
  double_quant: true

lora:
  rank: 64
  alpha: 128
  dropout: 0.05
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj
  bias: "none"
  task_type: "CAUSAL_LM"

training:
  optimizer: "paged_adamw_8bit"
  learning_rate: 2e-4
  lr_scheduler: "cosine"
  warmup_ratio: 0.03
  num_epochs: 6
  per_device_batch_size: 4
  gradient_accumulation_steps: 4
  max_seq_length: 512
  weight_decay: 0.01
  bf16: true

checkpointing:
  save_strategy: "epoch"
  save_path: "/vol/checkpoints/poet/"
  save_total_limit: 4

compute:
  gpu_14b: "A10G"
  gpu_32b: "A100-80GB"
  estimated_time_14b: "2 hours"
  estimated_cost_14b: "$2.75"
  estimated_time_32b: "4 hours"
  estimated_cost_32b: "$12.00"
