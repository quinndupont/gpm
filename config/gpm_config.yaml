project_name: "Good Poetry Model"
version: "1.0.0"

hardware:
  total_ram_gb: 24
  reserved_ram_gb: 4
  available_ram_gb: 20
  metal_enabled: true

ollama:
  model: "llama3.2:3b"
  host: "http://localhost:11434"
  temperature: 1.0  # Sawicki et al.: 1.0 for variability across runs (aggregate scores)
  max_tokens: 2000
  batch_size: 15    # Sawicki et al.: 15 poems optimal for CAT
  rate_limit_delay: 0.5

datasets:
  - name: "gutenberg"
    repo: "matthh/gutenberg-poetry-corpus"
    weight: 1.0
  - name: "merve"
    repo: "merve/poetry"
    weight: 1.0
  - name: "public_domain"
    repo: "DanFosing/public-domain-poetry"
    weight: 0.8
  - name: "modern"
    repo: "jassiyu/poetry-modern"
    weight: 1.0

processing:
  min_poem_length: 150
  max_poem_length: 8000
  min_lines: 4
  deduplication: true
  quality_filter: true
  use_offline: true  # Use HF cache without network (avoids 404/401 when corpuses cached)
  use_local_processed: true  # Load from data/processed/*_cleaned.jsonl when available
  dedup:
    split_method: word_ngram
    ngram_size: 13
    similarity_threshold: 0.8
    num_minhashes: 128

validation:
  min_analysis_length: 200
  min_word_count: 50
  check_repetition: true
  deduplicate_analyses: true
  dedup:
    split_method: word_ngram
    ngram_size: 13
    similarity_threshold: 0.8

training:
  base_model: "mlx-community/Llama-3.2-3B-Instruct-4bit"
  lora_rank: 8
  lora_alpha: 16
  num_layers: 16
  dropout: 0.0
  batch_size: 1
  iterations: 1000
  learning_rate: 0.00001
  steps_per_eval: 50
  save_every: 100
  max_seq_length: 2048
  gradient_checkpointing: true
