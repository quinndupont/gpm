# Local Inference â€” S4.2
# llama.cpp Metal backend on Mac Mini M4

educator:
  model_path: "./models/llama3.1-14b-educator-Q4_K_M.gguf"
  n_ctx: 4096
  n_threads: 8
  n_gpu_layers: -1
  use_mmap: true
  use_mlock: false
  generation_brief:
    temperature: 0.4
    top_p: 0.9
    repeat_penalty: 1.1
    max_tokens: 800
    stop: ["</s>", "<|eot_id|>"]
  critique:
    temperature: 0.3
    top_p: 0.9
    repeat_penalty: 1.1
    max_tokens: 600
    stop: ["</s>", "<|eot_id|>"]
  final_note:
    temperature: 0.3
    max_tokens: 400

poet:
  model_path: "./models/llama3.1-14b-poet-Q4_K_M.gguf"
  n_ctx: 2048
  n_threads: 8
  n_gpu_layers: -1
  use_mmap: true
  generation:
    temperature: 0.8
    top_p: 0.95
    repeat_penalty: 1.15
    max_tokens: 500
    stop: ["</s>", "<|eot_id|>"]
  revision:
    temperature: 0.75
    top_p: 0.9
    repeat_penalty: 1.1
    max_tokens: 500

performance:
  first_token_latency: "<100ms"
  sustained_throughput: ">=20 tokens/sec"
  model_load_time: "<30s per model"

max_revisions: 3
